# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17O1gnsDPG0PNSPkJSLf2QtQAgePj6f1g

Load and explore the data

Import libraries
"""

# Import pandas, numpy, and matplotlib
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# seaborn is a data visualization library built on matplotlib
import seaborn as sns

# set the plotting style
sns.set_style("whitegrid")

"""Load the Data

Load the Seattle data set
"""

df_seattle = pd.read_csv('https://raw.githubusercontent.com/Kaurgurpreet23/weather/refs/heads/main/data/seattle_rain.csv?token=GHSAT0AAAAAADL3NAZFWI3IPFJ3SPJUERS62GYOLWQ')

type(df_seattle)

"""Load the Vancouver Data Set file"""

df_vancouver=pd.read_csv('https://raw.githubusercontent.com/Kaurgurpreet23/weather/refs/heads/main/data/van_rain.csv.csv?token=GHSAT0AAAAAADL3NAZFBN3WFZ23PSWWRXIO2GYORZA')

"""## Explore the contents of the data sets

Start by looking at the head of each data frame.

This will let us see the names of the columns and a few example values for each column.
"""

df_seattle.head()

"""Examine more rows"""

df_seattle.head(10)

"""Vancouver Data Set"""

df_vancouver.head()

"""The columns are not entirely the same"""

df_seattle.columns

df_vancouver.columns

"""Use the info method to check the data types, size of the data frame, and numbers of missing values."""

df_seattle.info()

"""Vancover data set"""

df_vancouver.info()

"""We can also compare data frame sizes using the shape attribute"""

print(df_seattle.shape)

df_seattle.shape[0]

print(df_vancouver.shape)

"""Why might the Vancouver data set be larger?

Examine the STATION column
"""

df_vancouver['STATION']

"""How many unique stations are present?"""

df_vancouver['STATION'].unique()

df_vancouver['STATION'].nunique()

df_seattle['STATION'].nunique()

"""Examine the DATE column"""

df_vancouver['DATE']

df_seattle['DATE']

"""Be careful to use correct data types"""

df_seattle['DATE'].max()

df_seattle['DATE'].tail()

"""Convert DATE to datetime"""

df_seattle['DATE'] = pd.to_datetime(df_seattle['DATE'])

df_vancouver['DATE'] = pd.to_datetime(df_vancouver['DATE'])

df_seattle['DATE']

df_vancouver['DATE']

"""What range of dates are present?"""

df_seattle['DATE'].min()

df_seattle['DATE'].max()

df_seattle['DATE'].agg(['min', 'max'])

df_vancouver['DATE'].agg(['min', 'max'])

"""Are the data suitable for answering the question?

Plot the daily precipitation data for Seattle.
"""

plt.figure(figsize=(20, 5))

sns.lineplot(data=df_seattle, x='DATE', y='PRCP')

plt.xlabel('Date', fontsize=18)
plt.ylabel('Precipitation (inches)', fontsize=18)

plt.tick_params(labelsize=15)

plt.show()

df_seattle.head(20)

"""Plot the daily precipitation data for Vancouver"""

plt.figure(figsize=(20, 5))

sns.lineplot(data=df_vancouver, x='DATE', y='PRCP')

plt.xlabel('Date', fontsize=18)
plt.ylabel('Precipitation (inches)', fontsize=18)

plt.tick_params(labelsize=15)

plt.show()

"""Understanding the code"""

plt.figure(figsize=(20, 5))

sns.lineplot(data=df_seattle, x='DATE', y='PRCP')

plt.xlabel('Date', fontsize=18)
plt.ylabel('Precipitation (inches)', fontsize=18)

plt.tick_params(labelsize=15)

plt.show()

plt.figure(figsize=(20, 5))
sns.lineplot(data=df_seattle, x='DATE', y='PRCP')

"""Tutorial: Selecting subsets of a DataFrame"""

df_seattle.head()

"""Select one column

Select the column containing precipitation PRCP.

Use dictionary-style indexing
"""

df_seattle['PRCP']

"""Use explicit array-style indexing with loc"""

df_seattle.loc[:, 'PRCP']

"""Use implicit array-style indexing with .iloc

What number column, starting with 0, is PRCP?
"""

df_seattle.head()

"""
Use implicit array-style indexing with .iloc"""

df_seattle.iloc[:, 5]

"""Select multiple columns

Select the columns containing STATION, DATE, and precipitation PRCP.
"""

df_seattle[['STATION', 'DATE', 'PRCP']]

"""Use .loc"""

df_seattle.loc[:, ['STATION', 'DATE', 'PRCP']]

"""Use .iloc"""

df_seattle.iloc[:, [0, 2, 5]]

"""Select a range of columns

Select all columns from STATION through PRCP
"""

df_seattle.head()

df_seattle.loc[:, 'STATION':'PRCP']

"""Use .iloc"""

df_seattle.iloc[:, 0:6]

"""Select the first  n  rows

Select the first 3 rows

Use the .head() method
"""

df_seattle.head(3)

"""Use explicit indexing"""

df_seattle.loc[:2]

"""Use implicit indexing"""

df_seattle.iloc[:3]

"""Select the last  n  rows

Use the .tail() method
"""

df_seattle.tail(3)

"""Use explicit indexing"""

df_seattle.loc[len(df_seattle)-3:]

"""Use implicit indexing"""

df_seattle.iloc[-3:]

"""Select rows using logical indexing

Select the rows where PRCP is greater than 0.
"""

df_seattle['PRCP'] > 0

sum(df_seattle['PRCP'] > 0)

df_seattle[df_seattle['PRCP'] > 0]

"""Select the rows where the precipitation is between 0.5 and 0.75 inches.

"""

df_seattle[df_seattle['PRCP'].between(0.5, 0.75)]

df_seattle[(df_seattle['PRCP'] >= 0.5) & (df_seattle['PRCP'] <= 0.75)]

"""Select relevant subsets of the data

Join data frames keeping DATE and PRCP columns
"""

df_seattle.head(2)

df_vancouver.head(2)

"""What type of join should we do? We will do an outer join"""

df = df_vancouver[['DATE', 'PRCP']].merge(df_seattle[['DATE', 'PRCP']], on='DATE', how='outer')

df.head()

df.shape

"""Each DataFrame had a column named PRCP, so the default is to add suffixes _x and _y to differentiate the columns. Normally, I would rename the columns at this point to something more informative. However, I know that I want to convert the DataFrame to a tidy format and I will modify the names later.

Create a tidy data frame with columns for city and precipitation.
"""

df

df = pd.melt(df, id_vars='DATE', var_name='city', value_name='precipitation')

"""How did this change the DataFrame?"""

df.head()

df.tail()

"""# Rename columns or values to follow best practices

Rename the city values 'VAN' and 'SEA'
"""

df.loc[df['city'] == 'PRCP_x', 'city'] = 'VAN'

df.loc[df['city'] == 'PRCP_y', 'city'] = 'SEA'

df.head()

df.tail()

"""Rename the columns to be lowercase using df.rename()"""

df = df.rename(columns={'DATE': 'date'})

df.head()

"""# Identify and fill in missing values

Data can be missing in multiple manners:

1. Values are NaN in the data frame

2. Values are not included in the data set.

# Count the non-null or null values

Determine the number of non-null values in each colum
"""

df.info()

df.notna().sum()

"""Determine the number of null values in each column."""

df.isna().sum()

"""Determine the number of null precipitation values for Seattle and Vancouver"""

df.loc[df['city'] == 'SEA', 'precipitation'].isna().sum()

df.loc[df['city'] == 'VAN', 'precipitation'].isna().sum()

"""The Vancouver data set does have only three NaN values of precipitation. Are any dates omitted?
How many data points should we have from 2018 to 2022?

Over 5 years there should be
5Ã—365+1=1826

days.

The St. Louis data set is only any dates or precipitation values.

# Input missing values

We will replace missing values with the mean across years of values on that day.

Design an algorithm for replacing missing values with the mean across years of values on that day.

Define a column that labels each day by the day of the year: 1, 2, ..., 365.
"""

df['day_of_year'] = pd.DatetimeIndex(df['date']).day_of_year

df.head(10)

"""Compute the mean precipitation for each day in Seattle, averaged across years."""

mean_day_precipitation = df.loc[
    df['city'] == 'SEA',
    ['precipitation', 'day_of_year']
].groupby(
    'day_of_year'
).mean()

plt.figure(figsize=(20, 5))

sns.lineplot(data=mean_day_precipitation, x='day_of_year', y='precipitation')

plt.xlabel('Day of the year', fontsize=18)
plt.ylabel('Mean precipitation (inches)', fontsize=18)

plt.tick_params(labelsize=15)

plt.show()

"""Get the index of each row where precipitation is missing."""

indices = np.where(df['precipitation'].isna() == True)[0]

indices

"""Replace each missing value with the mean on that day"""

for index in indices:
    df.loc[index, 'precipitation'] = mean_day_precipitation.loc[df.loc[index,'day_of_year']].values[0]

"""Check for missing values in the data frame"""

df.isna().sum()

"""Export the clean .csv file"""

df.to_csv('clean_seattle_vancouver_weather.csv', encoding='utf-8-sig', index=False)